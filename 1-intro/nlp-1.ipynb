{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xwJ5Q6w2IFHA"
      },
      "source": [
        "**Ссылка**, на источник текста"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "64uxt-07IEec"
      },
      "source": [
        "DATA_URL = 'http://az.lib.ru/g/gogolx_n_w/text_0050.shtml'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gwrBkeLnHuA1"
      },
      "source": [
        "Устанавливаем библиотеки"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H5Fr5swwYfz5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bbf42919-eb4e-4454-f995-c78bdcf1d1d2"
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "!pip install -q tensorflow==2.12\n",
        "!pip install -q git+https://github.com/dvolchek/rnnmorph.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m585.9/585.9 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m37.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m54.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m72.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m440.7/440.7 kB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "albucore 0.0.15 requires numpy>=1.24, but you have numpy 1.23.5 which is incompatible.\n",
            "albumentations 1.4.15 requires numpy>=1.24.4, but you have numpy 1.23.5 which is incompatible.\n",
            "bigframes 1.17.0 requires numpy>=1.24.0, but you have numpy 1.23.5 which is incompatible.\n",
            "chex 0.1.86 requires numpy>=1.24.1, but you have numpy 1.23.5 which is incompatible.\n",
            "pandas-stubs 2.1.4.231227 requires numpy>=1.26.0; python_version < \"3.13\", but you have numpy 1.23.5 which is incompatible.\n",
            "tf-keras 2.17.0 requires tensorflow<2.18,>=2.17, but you have tensorflow 2.12.0 which is incompatible.\n",
            "xarray 2024.9.0 requires numpy>=1.24, but you have numpy 1.23.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.5/55.5 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.2/8.2 MB\u001b[0m \u001b[31m49.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for rnnmorph (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for russian-tagsets (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gbxMKqhPH1Dk"
      },
      "source": [
        "Создаём объект морфологического анализатора `RNNMorph`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "24zMUhvi99AV"
      },
      "source": [
        "from rnnmorph.predictor import RNNMorphPredictor\n",
        "predictor = RNNMorphPredictor(language=\"ru\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59q1L9p0H9K9"
      },
      "source": [
        "Скачиваем текст, по которому будет дано задание, с помощью `urllib`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0uW0fw_h-Pft"
      },
      "source": [
        "import urllib.request\n",
        "\n",
        "opener = urllib.request.URLopener({})\n",
        "resource = opener.open(DATA_URL)\n",
        "raw_text = resource.read().decode(resource.headers.get_content_charset()) #Текс с html тегами"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "-hSPOjDo4sdh",
        "outputId": "633b4dc4-4cd1-4000-ea95-19ada7fb00bf"
      },
      "source": [
        "raw_text[:200]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<html>\\r\\n<head>\\r\\n<title>Lib.ru/Классика: Гоголь Николай Васильевич. Вий</title>\\r\\n</head>\\r\\n\\r\\n<body>\\r\\n\\r\\n\\r\\n<center>\\r\\n\\r\\n<h2><a href=/g/gogolx_n_w/>Гоголь Николай Васильевич</a><br>\\r\\nВий</h2>\\r\\n\\r\\n<!------- П'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uZiLHQNSITAt"
      },
      "source": [
        "Как видно, текст содержит html теги, от которых нужно избавиться. Выбрасываем из текста HTML-теги с помощью библиотеки Beatiful soap"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "We4LkyUMPfuq"
      },
      "source": [
        "from bs4 import BeautifulSoup\n",
        "soup = BeautifulSoup(raw_text, features=\"html.parser\")\n",
        "\n",
        "# kill all script and style elements\n",
        "for script in soup([\"script\", \"style\"]):\n",
        "    script.extract()    # rip it out\n",
        "\n",
        "# get text\n",
        "cleaned_text = soup.get_text()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "lOD8PJnG4rbl",
        "outputId": "12d8bdf5-3ffc-40d4-8a5b-5eb2ac950092"
      },
      "source": [
        "cleaned_text[:200]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n\\nLib.ru/Классика: Гоголь Николай Васильевич. Вий\\n\\n\\n\\nГоголь Николай Васильевич\\r\\nВий\\n\\n\\nLib.ru/Классика:\\n\\r\\n\\n\\n[Регистрация]\\n \\n\\r\\n\\r\\n\\r\\n[Найти] \\r\\n[Рейтинги]\\r\\n[Обсуждения]\\r\\n[Новинки]\\r\\n[Обзоры]\\r\\n[Помощь]\\r\\n\\r\\n\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14fYYb5hIpnY"
      },
      "source": [
        "С помощью библиотеки [NLTK](https://nltk.org/) разбиваем текст на предложения и токены."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "hRNu7jPvN6G_",
        "outputId": "6942786f-736e-46b6-c8f4-7b122c3b7755"
      },
      "source": [
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "tokenized_sentences = [word_tokenize(sentence) for sentence in sent_tokenize(cleaned_text)]\n",
        "\"A total of %d 'sentences'\" % len(tokenized_sentences)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"A total of 934 'sentences'\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xRU4KEBAIyYT"
      },
      "source": [
        "## Задание 1\n",
        "С помощью метода `str.isalpha` из стандартной библиотеки Python модифицируйте нижеследующий код так, чтобы в predictions остались только буквенные токены."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4U5HH2CDPVUM",
        "outputId": "94c6c9b9-4f24-48b5-955e-336d6703757c"
      },
      "source": [
        "from tqdm import tqdm\n",
        "predictions = [[pred.normal_form for pred in sent]\n",
        "               for sent in tqdm(predictor.predict_sentences(sentences=tokenized_sentences), \"sentences\") ]\n",
        "predictions[-11:-10] #Сейчас видно, что токены типа \"точка\", \"запятая\" и тд пока присутствуют в предложениях. От них нужно избавиться"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "15/15 [==============================] - 8s 432ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "sentences: 100%|██████████| 934/934 [00:00<00:00, 232546.59it/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['пфейфер', '(', 'он', '.', ')']]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = [[token for token in sent if token.isalpha()] for sent in predictions]"
      ],
      "metadata": {
        "id": "bBv0hyn7z3iD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nwK_qRbw6sac",
        "outputId": "fbdba8a3-933b-494d-f9c7-53e3a795d89e"
      },
      "source": [
        "print(f'Количество предложений: {len(predictions)}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Количество предложений: 934\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J5jL4sWyKUnO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "88800a68-e7d2-4e29-b43b-1402b700f00c"
      },
      "source": [
        "non_uniq_tokens = [word for sentence in predictions for word in sentence]\n",
        "print(f'Количество токенов: {len(non_uniq_tokens)}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Количество токенов: 11732\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mci9Nd5hKuJP"
      },
      "source": [
        "## Задание 2\n",
        "\n",
        "Используя `non_uniq_tokens`, стоп-слова для русского языка из библиотеки nltk (`nltk.corpus.stopwords`) и `nltk.FreqDist`, вычислите, **какую долю среди 50 самых частотных** токенов в произведении занимают токены, **не относящиеся** к стоп словам.\n",
        "\n",
        "**Например**, если среди 100 самых частотных слов встречается 25 слов, входящих в стоп лист, значит не входят в стоп лист 75 слов, и их доля составит 0.75."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gHbtLqkLKfZC",
        "outputId": "fe6f92f2-d562-4cd0-cb3c-dfac7b6c58b7"
      },
      "source": [
        "import nltk\n",
        "from nltk import FreqDist\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download(\"stopwords\")\n",
        "STOPWORDS = set(stopwords.words(\"russian\"))\n",
        "stopwords.words(\"russian\")[:5] #Пример стоп слов"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['и', 'в', 'во', 'не', 'что']"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# получим 50 самых частотных токенов\n",
        "fdist = FreqDist(word.lower() for word in non_uniq_tokens)\n",
        "most_common_50 = fdist.most_common(50)\n",
        "most_common_50"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XHVGsechw8rM",
        "outputId": "a294f799-abb3-41ec-9c46-fe9b1d4cce1d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('он', 553),\n",
              " ('и', 528),\n",
              " ('в', 322),\n",
              " ('на', 214),\n",
              " ('не', 191),\n",
              " ('быть', 191),\n",
              " ('что', 189),\n",
              " ('с', 136),\n",
              " ('философ', 125),\n",
              " ('как', 122),\n",
              " ('весь', 95),\n",
              " ('свой', 93),\n",
              " ('я', 85),\n",
              " ('по', 79),\n",
              " ('сказать', 69),\n",
              " ('а', 68),\n",
              " ('из', 66),\n",
              " ('к', 63),\n",
              " ('только', 61),\n",
              " ('за', 57),\n",
              " ('тот', 57),\n",
              " ('один', 56),\n",
              " ('но', 56),\n",
              " ('такой', 55),\n",
              " ('бы', 55),\n",
              " ('который', 54),\n",
              " ('или', 53),\n",
              " ('ты', 53),\n",
              " ('уже', 52),\n",
              " ('то', 50),\n",
              " ('у', 46),\n",
              " ('же', 45),\n",
              " ('это', 45),\n",
              " ('сам', 44),\n",
              " ('так', 42),\n",
              " ('другой', 40),\n",
              " ('когда', 40),\n",
              " ('да', 39),\n",
              " ('себя', 38),\n",
              " ('рука', 38),\n",
              " ('хома', 38),\n",
              " ('этот', 36),\n",
              " ('всё', 36),\n",
              " ('ни', 35),\n",
              " ('знать', 34),\n",
              " ('мочь', 34),\n",
              " ('о', 33),\n",
              " ('глаз', 32),\n",
              " ('говорить', 30),\n",
              " ('начать', 29)]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# получим токены, не относящиеся к стоп-словам\n",
        "non_stopword_tokens = [token for token, frequency in most_common_50 if token not in STOPWORDS]\n",
        "\n",
        "# рассчитаем долю\n",
        "non_stopword_tokens_fraction = len(non_stopword_tokens) / 50\n",
        "print(f'Доля токенов, не относящихся к стоп словам среди 50 самых частотных токенов: {non_stopword_tokens_fraction:.2f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jx6GZ6ZKyNMz",
        "outputId": "ffa53c5f-8101-424d-ac44-3fb38f7bbe0a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Доля токенов, не относящихся к стоп словам среди 50 самых частотных токенов: 0.28\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HChyAdk2Ovx1"
      },
      "source": [
        "## Задание 3\n",
        "Вычислите, сколько токенов встречается в тексте **строго больше** 50 раз."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokens_above_50 = [token for token, frequency in fdist.items() if frequency > 50]\n",
        "tokens_above_50_count = len(tokens_above_50)\n",
        "print(f'Кол-во токенов, встречающихся в тексте строго больше 50 раз: {tokens_above_50_count}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wlkr_M5EzOYr",
        "outputId": "6ebff4d5-9d83-46e5-c1fc-0ae207401779"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Кол-во токенов, встречающихся в тексте строго больше 50 раз: 29\n"
          ]
        }
      ]
    }
  ]
}